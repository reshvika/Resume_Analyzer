{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pdf2image\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\reshv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfplumber) (11.1.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\reshv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20250327->pdfplumber)\n",
      "  Downloading cryptography-44.0.2-cp39-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\reshv\\appdata\\roaming\\python\\python312\\site-packages (from pytesseract) (24.1)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber)\n",
      "  Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.6 MB 16.4 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.8/5.6 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 2.9/5.6 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.2/5.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 6.2 MB/s eta 0:00:00\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 2.4/3.0 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 11.5 MB/s eta 0:00:00\n",
      "Downloading cryptography-44.0.2-cp39-abi3-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 2.6/3.2 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 11.8 MB/s eta 0:00:00\n",
      "Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pytesseract, pypdfium2, pycparser, pdf2image, cffi, cryptography, pdfminer.six, pdfplumber\n",
      "Successfully installed cffi-1.17.1 cryptography-44.0.2 pdf2image-1.17.0 pdfminer.six-20250327 pdfplumber-0.11.6 pycparser-2.22 pypdfium2-4.30.1 pytesseract-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script pytesseract.exe is installed in 'c:\\Users\\reshv\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pypdfium2.exe is installed in 'c:\\Users\\reshv\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pdfplumber.exe is installed in 'c:\\Users\\reshv\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfplumber pytesseract pdf2image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get The text from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Try direct text extraction\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        if text.strip():\n",
    "            return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Direct text extraction failed: {e}\")\n",
    "\n",
    "    # Fallback to OCR for image-based PDFs\n",
    "    print(\"Falling back to OCR for image-based PDF.\")\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path)\n",
    "        for image in images:\n",
    "            page_text = pytesseract.image_to_string(image)\n",
    "            text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"OCR failed: {e}\")\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Text from PDF:\n",
      "GADE MARIA RESVIKA REDDY\n",
      "P: +91 123456789 | mariareshvika2005@gmail.com| github.com/reshvika| www.linkedin.com/in/reshvikareddy\n",
      "EDUCATION\n",
      "ANURAG UNIVERSITY Hyderabad, India\n",
      "Bachelor of Engineering Sep 2022 – May 2026\n",
      "Major in Artificial Intelligence\n",
      "Cumulative GPA: 8.12/10.0\n",
      "Relevant Coursework: Data Analysis, Machine Learning, Deep Learning, Software Engineering; Operating Systems;\n",
      "Algorithms; Artificial Intelligence\n",
      "WORK EXPERIENCE\n",
      "CORIZO (ed-techstartup) Hyderabd, India\n",
      "Marketing Intern Oct 2023 – Nov 2023\n",
      "● Managed online and campus marketing campaigns (social media, email, web, on-campus events) and tracked Reach Rate to\n",
      "improve visibility.\n",
      "● Created engaging content that increased social media Engagement Rate by 25%, and designed promotional materials for\n",
      "campus events.\n",
      "● Supported brand awareness initiatives and coordinated both online and on-campus promotional events, focusing on\n",
      "maximizing Marketing ROI.\n",
      "UNIVERSITY PROJECTS\n",
      "SUPERSTORE SALES DASHBOARD Feb 2025\n",
      "● Developed an interactive dashboard to analyze sales performance and trends.\n",
      "● Visualized key sales metrics, including 1.6M total sales and 22K quantity sold.\n",
      "● Tracked sales and profit trends for 2019 and 2020 across key categories (Office Supplies, Technology, Furniture).\n",
      "● Compared sales and ana;lyzed delivery time and payment modes,performance across shipping modes.\n",
      "HEART DISEASE PREDICTION Oct 2024\n",
      "● Developed and compared machine learning classifiers (K-Nearest Neighbors, Decision Tree, Random Forest) for heart\n",
      "disease prediction, achieving up to 84.81% accuracy.\n",
      "● Implemented data preprocessing and normalization techniques to optimize medical datasets for machine learning model\n",
      "training, enhancing prediction accuracy.\n",
      "● Measured model accuracy, resulting in 84.48% for K-Nearest Neighbors and 84.81% for Random Forest, showcasing\n",
      "analytical proficiency.\n",
      "● Used Python (Pandas, NumPy, Scikit-learn) for data analysis and model developmentACTIVITIES\n",
      "TECHAMUSE CLUB Hyderabad, India\n",
      "Head of Content Jul 2024 – Present\n",
      "● Managed content strategy and event planning as Head of Content, ensuring consistent quality and effective communication\n",
      "to a large student audience.\n",
      "● Organized and advertised 5+ quarterly networking events with 200+ participants in campus\n",
      "KRIYA PASSIONATE EVENTS Hyderabad, India Committee Member Aug 2023 – Jan 2025\n",
      "● Managed volunteer teams, providing clear direction and fostering a collaborative environment.\n",
      "TEDXHYDERABAD Hyderabad, India\n",
      "Volunteer Dec 2024\n",
      "• Oversaw the registration desk at TEDx Hyderabad, processing attendee check-in efficiently and providing service\n",
      "ADDITIONAL\n",
      "Programming and Technical Skills:Python,MySQL,R,Power BI,PySpark, Java, C++, TensorFlow, PyTorch, Scikit-Learn,\n",
      "OpenCV, Pandas\n",
      "Math & Analytics: Probability, Statistics, Business Acumen, Data Analysis\n",
      "Certifications & Training: IBM Data Visualization, AWS,Generative AI Linkedin, Cisco CCNA\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"Resume.pdf\"\n",
    "resume_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(\"\\nExtracted Text from PDF:\")\n",
    "print(resume_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Set Google GenerativeAI Api Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google.generativeaiNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\reshv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.1)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google.generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google.generativeai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google.generativeai)\n",
      "  Downloading google_api_python_client-2.166.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting google-auth>=2.15.0 (from google.generativeai)\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting protobuf (from google.generativeai)\n",
      "  Downloading protobuf-6.30.2-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting pydantic (from google.generativeai)\n",
      "  Downloading pydantic-2.11.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting tqdm (from google.generativeai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions (from google.generativeai)\n",
      "  Downloading typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google.generativeai)\n",
      "  Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google.generativeai)\n",
      "  Downloading googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\reshv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google.generativeai) (2.32.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google.generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google.generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google.generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->google.generativeai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.0 (from pydantic->google.generativeai)\n",
      "  Downloading pydantic_core-2.33.0-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic->google.generativeai)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\reshv\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->google.generativeai) (0.4.6)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google.generativeai)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\reshv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\reshv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\reshv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\reshv\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2025.1.31)\n",
      "Downloading google_generativeai-0.8.4-py3-none-any.whl (175 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 11.5 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading google_api_python_client-2.166.0-py2.py3-none-any.whl (13.2 MB)\n",
      "   ---------------------------------------- 0.0/13.2 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 3.9/13.2 MB 19.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 6.0/13.2 MB 14.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.7/13.2 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.2 MB 15.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.2/13.2 MB 14.0 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.11.1-py3-none-any.whl (442 kB)\n",
      "Downloading pydantic_core-2.33.0-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 18.0 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ---------------------------------------  4.2/4.3 MB 21.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 17.1 MB/s eta 0:00:00\n",
      "Downloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: uritemplate, typing-extensions, tqdm, pyparsing, pyasn1, protobuf, grpcio, cachetools, annotated-types, typing-inspection, rsa, pydantic-core, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, pydantic, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google.generativeai\n",
      "Successfully installed annotated-types-0.7.0 cachetools-5.5.2 google-ai-generativelanguage-0.6.15 google-api-core-2.24.2 google-api-python-client-2.166.0 google-auth-2.38.0 google-auth-httplib2-0.2.0 google.generativeai-0.8.4 googleapis-common-protos-1.69.2 grpcio-1.71.0 grpcio-status-1.71.0 httplib2-0.22.0 proto-plus-1.26.1 protobuf-5.29.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.1 pydantic-core-2.33.0 pyparsing-3.2.3 rsa-4.9 tqdm-4.67.1 typing-extensions-4.13.0 typing-inspection-0.4.0 uritemplate-4.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\reshv\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'c:\\Users\\reshv\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install google.generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reshv\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(\"What is the capital of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"The capital of India is **New Delhi**.\\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"avg_logprobs\": -0.0026348402723670008\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 7,\n",
      "        \"candidates_token_count\": 10,\n",
      "        \"total_token_count\": 17\n",
      "      },\n",
      "      \"model_version\": \"gemini-1.5-flash\"\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is **New Delhi**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resume Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_resume(resume_text, job_description=None):\n",
    "    if not resume_text:\n",
    "        return {\"error\": \"Resume text is required for analysis.\"}\n",
    "    \n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    \n",
    "    base_prompt = f\"\"\"\n",
    "    You are an experienced HR with Technical Experience in the field of any one job role from Data Science, Data Analyst, DevOPS, Machine Learning Engineer, Prompt Engineer, AI Engineer, Full Stack Web Development, Big Data Engineering, Marketing Analyst, Human Resource Manager, Software Developer your task is to review the provided resume.\n",
    "    Please share your professional evaluation on whether the candidate's profile aligns with the role.ALso mention Skills he already have and siggest some skills to imorve his resume , alos suggest some course he might take to improve the skills.Highlight the strengths and weaknesses.\n",
    "\n",
    "    Resume:\n",
    "    {resume_text}\n",
    "    \"\"\"\n",
    "\n",
    "    if job_description:\n",
    "        base_prompt += f\"\"\"\n",
    "        Additionally, compare this resume to the following job description:\n",
    "        \n",
    "        Job Description:\n",
    "        {job_description}\n",
    "        \n",
    "        Highlight the strengths and weaknesses of the applicant in relation to the specified job requirements.\n",
    "        \"\"\"\n",
    "\n",
    "    response = model.generate_content(base_prompt)\n",
    "\n",
    "    analysis = response.text.strip()\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Resume Evaluation for Gade Maria Resvika Reddy\n",
      "\n",
      "**Role Assumed for Evaluation:**  Data Scientist (Given the candidate's coursework and projects, this is the most appropriate role for evaluation.)\n",
      "\n",
      "**Overall Assessment:**\n",
      "\n",
      "The resume demonstrates a promising start for a recent graduate aiming for a Data Scientist role.  The candidate showcases relevant projects and skills, especially for a student. However, there are areas for improvement to enhance competitiveness.\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "* **Relevant Projects:** The Superstore Sales Dashboard and Heart Disease Prediction projects are strong highlights, demonstrating practical application of data analysis and machine learning techniques.  The metrics provided (accuracy, sales figures) are commendable.\n",
      "* **Diverse Skillset:** The candidate possesses a good range of technical skills including Python (with relevant libraries), R, SQL, Power BI, and experience with cloud platforms (AWS).\n",
      "* **Extracurricular Activities:**  Leadership roles in Techamuse Club and other organizations demonstrate teamwork, communication, and organizational skills – valuable assets for a data scientist.\n",
      "* **Clear Structure:** The resume is well-organized and easy to read.\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "* **Limited Work Experience:**  The marketing internship, while valuable, offers limited direct experience in data science.  The resume would benefit significantly from more substantial, data-focused work experience, even if it's through personal projects or volunteer work.\n",
      "* **Underdeveloped Project Descriptions:** While the projects are mentioned, they lack detail.  Expand on the challenges faced, methodologies used, results achieved, and any insights gained. Quantify achievements whenever possible (e.g., \"Improved model accuracy by X% by implementing Y technique\").\n",
      "* **Missing Quantifiable Results in Internship:** The marketing internship mentions increased engagement, but lacks specific numbers or KPIs beyond percentages.  Adding quantifiable results (e.g., \"Increased social media engagement rate by 25%, resulting in a 10% increase in lead generation\") will strengthen this section.\n",
      "* **Incomplete Education Details:** The resume states the graduation date as May 2026.  Adding expected graduation date and detailing any relevant thesis or capstone project would strengthen this section.\n",
      "* **Unclear Certification Details:** The certifications listed (IBM Data Visualization, AWS, Generative AI Linkedin, Cisco CCNA) lack specifics. Mentioning the specific certifications earned would be more impactful (e.g., \"AWS Certified Cloud Practitioner\").\n",
      "\n",
      "\n",
      "**Skills the Candidate Already Has:**\n",
      "\n",
      "* **Programming Languages:** Python, R, Java, C++\n",
      "* **Data Analysis & Visualization:** Pandas, NumPy, Scikit-learn, Power BI, Data Analysis, Data Visualization\n",
      "* **Machine Learning:**  K-Nearest Neighbors, Decision Tree, Random Forest, TensorFlow, PyTorch\n",
      "* **Databases:** MySQL\n",
      "* **Big Data Technologies:** PySpark\n",
      "* **Cloud Computing:** AWS (needs more specifics)\n",
      "* **Other:** OpenCV, Probability, Statistics, Business Acumen\n",
      "\n",
      "**Skills to Improve:**\n",
      "\n",
      "* **Advanced Machine Learning Techniques:** Explore deep learning models (RNNs, CNNs, transformers) and more advanced techniques like model tuning, hyperparameter optimization, and ensemble methods.\n",
      "* **Data Wrangling & Preprocessing:** Strengthen skills in handling missing data, outliers, and feature engineering.\n",
      "* **Big Data Ecosystem:** Gain deeper experience with Hadoop, Spark, and other big data technologies beyond PySpark.\n",
      "* **Cloud Platforms:**  Get more specific AWS certifications and potentially explore other cloud providers (Azure, GCP).\n",
      "* **Version Control:** Proficiency in Git and GitHub is essential. While the candidate lists GitHub, highlight specific contributions.\n",
      "* **Communication & Presentation Skills:**  Practice clearly articulating technical concepts to both technical and non-technical audiences.\n",
      "\n",
      "\n",
      "**Suggested Courses:**\n",
      "\n",
      "* **Advanced Machine Learning Specialization (Coursera, edX):** To delve into deep learning and advanced algorithms.\n",
      "* **Data Wrangling with Python (DataCamp, Coursera):** To enhance data preprocessing and cleaning skills.\n",
      "* **Big Data Fundamentals (various platforms):** To broaden knowledge of the Hadoop ecosystem and Spark.\n",
      "* **AWS Certified Cloud Practitioner Certification (AWS Training):** To gain a recognized cloud computing certification.\n",
      "* **Data Storytelling with Tableau/Power BI (various platforms):**  To improve visualization and communication skills.\n",
      "* **Technical Communication for Data Scientists (LinkedIn Learning, Coursera):**  To strengthen presentation skills.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Gade Maria Resvika Reddy has a solid foundation and potential as a data scientist.  By addressing the weaknesses outlined above, especially gaining more experience (through internships, personal projects, or volunteer work) and elaborating on the accomplishments in existing projects, the candidate can significantly improve their resume and increase their competitiveness in the job market.  Focusing on acquiring advanced machine learning skills and further developing their expertise in cloud computing and big data technologies will further enhance their profile.\n"
     ]
    }
   ],
   "source": [
    "print(analyze_resume(resume_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
